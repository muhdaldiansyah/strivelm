{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn GPT Architecture - StriveLM\n",
    "\n",
    "An educational walkthrough of building a character-level GPT from scratch.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/muhdaldiansyah/strivelm/blob/main/learn_gpt.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Token & Positional Embeddings** - How text becomes numbers\n",
    "2. **Transformer Architecture** - Self-attention mechanism\n",
    "3. **Multi-Head Attention** - Query, Key, Value (QKV)\n",
    "4. **Layer Normalization** - Stabilizing training\n",
    "5. **Residual Connections** - Gradient flow\n",
    "6. **Feed-Forward Networks** - Processing representations\n",
    "7. **Training & Generation** - Making your model talk!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/muhdaldiansyah/strivelm.git\n",
    "%cd strivelm\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify PyTorch and GPU\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understanding the Data\n",
    "\n",
    "GPT is a **character-level language model** - it learns patterns at the character level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and inspect the training data\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset size: {len(text)} characters\")\n",
    "print(f\"\\nFirst 300 characters:\\n{'-'*60}\")\n",
    "print(text[:300])\n",
    "print(f\"{'-'*60}\\n\")\n",
    "\n",
    "# Build vocabulary\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size} unique characters\")\n",
    "print(f\"Characters: {''.join(chars[:50])}{'...' if len(chars) > 50 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Architecture Deep Dive\n",
    "\n",
    "Let's explore the GPT architecture component by component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Token Embedding\n",
    "\n",
    "**Purpose:** Convert discrete tokens (characters) into continuous vectors.\n",
    "\n",
    "```\n",
    "Input: 'H' → Token ID: 12\n",
    "       ↓\n",
    "Embedding Layer (vocab_size × n_embd)\n",
    "       ↓\n",
    "Output: [0.23, -0.15, 0.89, ...] (128-dim vector)\n",
    "```\n",
    "\n",
    "Each character gets its own learned vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Token Embedding\n",
    "from model import GPT\n",
    "\n",
    "# Load the model to inspect\n",
    "model = GPT(vocab_size=vocab_size, block_size=64, n_layer=2, n_head=2, n_embd=128)\n",
    "print(\"Token Embedding Layer:\")\n",
    "print(f\"  Shape: {model.tok.weight.shape}\")\n",
    "print(f\"  Meaning: {vocab_size} tokens × {128} dimensions\")\n",
    "print(f\"\\n  Example: Character 'T' → 128-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Positional Embedding\n",
    "\n",
    "**Purpose:** Add position information since transformers have no inherent notion of sequence order.\n",
    "\n",
    "```\n",
    "Position 0: [0.12, 0.45, -0.23, ...]\n",
    "Position 1: [0.34, -0.12, 0.56, ...]\n",
    "Position 2: [0.89, 0.23, -0.67, ...]\n",
    "...\n",
    "```\n",
    "\n",
    "**Formula:** `final_embedding = token_embedding + position_embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Positional Embedding\n",
    "print(\"Positional Embedding Layer:\")\n",
    "print(f\"  Shape: {model.pos.weight.shape}\")\n",
    "print(f\"  Meaning: {64} positions × {128} dimensions\")\n",
    "print(f\"\\n  The model can handle sequences up to 64 tokens long\")\n",
    "print(f\"  Each position gets a unique learned vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Head Self-Attention (The Core of Transformers)\n",
    "\n",
    "**Purpose:** Let each token \"attend to\" (look at) other tokens in the sequence.\n",
    "\n",
    "#### QKV Mechanism:\n",
    "\n",
    "```\n",
    "Input: \"Hello\"\n",
    "       ↓\n",
    "For each token, compute 3 vectors:\n",
    "  Q (Query):  \"What am I looking for?\"\n",
    "  K (Key):    \"What do I contain?\"\n",
    "  V (Value):  \"What do I output?\"\n",
    "       ↓\n",
    "Attention(Q,K,V) = softmax(QK^T / √d) × V\n",
    "       ↓\n",
    "Output: Context-aware representations\n",
    "```\n",
    "\n",
    "#### Example:\n",
    "When processing \"**cat**\", the model might attend to:\n",
    "- Previous word \"the\" (70% attention)\n",
    "- Itself \"cat\" (20% attention)\n",
    "- Next word \"sat\" (10% attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: QKV Projection\n",
    "block = model.blocks[0]  # First transformer block\n",
    "print(\"Multi-Head Attention:\")\n",
    "print(f\"  Number of heads: {block.h}\")\n",
    "print(f\"  Head dimension: {block.hd}\")\n",
    "print(f\"  QKV projection shape: {block.qkv.weight.shape}\")\n",
    "print(f\"\\n  The input (128-dim) is projected to:\")\n",
    "print(f\"    Q: 128 dimensions (queries)\")\n",
    "print(f\"    K: 128 dimensions (keys)\")\n",
    "print(f\"    V: 128 dimensions (values)\")\n",
    "print(f\"  Then split into {block.h} heads of {block.hd} dims each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Layer Normalization\n",
    "\n",
    "**Purpose:** Normalize activations to stabilize training.\n",
    "\n",
    "```\n",
    "LayerNorm(x) = γ × (x - μ) / σ + β\n",
    "```\n",
    "\n",
    "Where:\n",
    "- μ = mean of features\n",
    "- σ = standard deviation\n",
    "- γ, β = learnable parameters\n",
    "\n",
    "**Why?** Prevents internal covariate shift and allows deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Layer Normalization\n",
    "print(\"Layer Normalization:\")\n",
    "print(f\"  Pre-attention LayerNorm: {block.ln1}\")\n",
    "print(f\"  Pre-FFN LayerNorm: {block.ln2}\")\n",
    "print(f\"\\n  Applied BEFORE each sub-layer (Pre-LN architecture)\")\n",
    "print(f\"  Normalizes across the embedding dimension (128)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Residual Connections\n",
    "\n",
    "**Purpose:** Allow gradients to flow directly through the network.\n",
    "\n",
    "```\n",
    "x_out = x + SubLayer(LayerNorm(x))\n",
    "```\n",
    "\n",
    "**Visualization:**\n",
    "```\n",
    "     Input (x)\n",
    "       |\n",
    "       ├──→ LayerNorm → Attention → ──┐\n",
    "       |                              |\n",
    "       └──────────────────────────────┴→ Add → Output\n",
    "```\n",
    "\n",
    "**Why?** Solves vanishing gradient problem in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the forward pass\n",
    "print(\"Transformer Block Forward Pass:\")\n",
    "print(\"\\n1. x = input\")\n",
    "print(\"2. x = x + Dropout(Attention(LayerNorm(x)))    ← Residual #1\")\n",
    "print(\"3. x = x + Dropout(FFN(LayerNorm(x)))          ← Residual #2\")\n",
    "print(\"4. return x\")\n",
    "print(\"\\nThe input can flow directly to the output via '+' operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Feed-Forward Network (FFN)\n",
    "\n",
    "**Purpose:** Process each position independently with non-linearity.\n",
    "\n",
    "```\n",
    "FFN(x) = Linear₂(GELU(Linear₁(x)))\n",
    "       = W₂ × GELU(W₁ × x + b₁) + b₂\n",
    "```\n",
    "\n",
    "**Architecture:**\n",
    "- Expand: 128 → 512 (4x expansion)\n",
    "- Activation: GELU (smoother than ReLU)\n",
    "- Compress: 512 → 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Feed-Forward Network\n",
    "print(\"Feed-Forward Network:\")\n",
    "print(f\"  Architecture: {block.ff}\")\n",
    "print(f\"\\n  Layer 1: 128 → 512 (4x expansion)\")\n",
    "print(f\"  GELU activation (smooth, non-linear)\")\n",
    "print(f\"  Layer 2: 512 → 128 (back to original)\")\n",
    "print(f\"\\n  Applied to each token independently\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Dropout\n",
    "\n",
    "**Purpose:** Regularization to prevent overfitting.\n",
    "\n",
    "```\n",
    "During training: Randomly zero out p% of neurons\n",
    "During inference: Use all neurons, scale by (1-p)\n",
    "```\n",
    "\n",
    "**Why?** Forces network to learn robust features, not memorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dropout rate\n",
    "from config import Config\n",
    "cfg = Config()\n",
    "print(f\"Dropout rate: {cfg.dropout}\")\n",
    "print(f\"\\nDropout = {cfg.dropout} means:\")\n",
    "if cfg.dropout == 0.0:\n",
    "    print(\"  No dropout (all neurons active)\")\n",
    "    print(\"  Good for small datasets where we want to learn everything\")\n",
    "else:\n",
    "    print(f\"  {cfg.dropout*100}% of neurons randomly zeroed during training\")\n",
    "    print(f\"  Helps prevent overfitting on large datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Complete Architecture Summary\n",
    "\n",
    "```\n",
    "Input Text: \"Hello\"\n",
    "     ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ Token Embedding (vocab → n_embd)   │  ← Convert chars to vectors\n",
    "│        +                            │\n",
    "│ Positional Embedding (pos → n_embd)│  ← Add position info\n",
    "└─────────────────────────────────────┘\n",
    "     ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ Transformer Block 1                 │\n",
    "│   ├─ LayerNorm                      │\n",
    "│   ├─ Multi-Head Attention (QKV)    │  ← Tokens look at each other\n",
    "│   ├─ Residual Connection            │\n",
    "│   ├─ LayerNorm                      │\n",
    "│   ├─ Feed-Forward Network (FFN)    │  ← Process representations\n",
    "│   └─ Residual Connection            │\n",
    "└─────────────────────────────────────┘\n",
    "     ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ Transformer Block 2 (same structure)│\n",
    "└─────────────────────────────────────┘\n",
    "     ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ Final LayerNorm                     │\n",
    "└─────────────────────────────────────┘\n",
    "     ↓\n",
    "┌─────────────────────────────────────┐\n",
    "│ Linear Head (n_embd → vocab_size)  │  ← Predict next character\n",
    "└─────────────────────────────────────┘\n",
    "     ↓\n",
    "Output: Probability distribution over vocab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print full model architecture\n",
    "print(\"Complete Model Architecture:\")\n",
    "print(\"=\"*60)\n",
    "print(model)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n",
    "\n",
    "Now let's train your GPT! The model learns to predict the next character given previous characters.\n",
    "\n",
    "**Training objective:** Minimize cross-entropy loss\n",
    "```\n",
    "Loss = -log(P(correct_next_char | context))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\\n\")\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Training Output\n",
    "\n",
    "```\n",
    "iter 1   | train 3.735 | val 3.826\n",
    "```\n",
    "\n",
    "- **iter**: Training iteration\n",
    "- **train loss**: Lower = better predictions on training data\n",
    "- **val loss**: Lower = better generalization to unseen data\n",
    "\n",
    "**Loss starts high (~3.7)** because the model is guessing randomly.\n",
    "\n",
    "**Loss decreases** as the model learns character patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Generate Text (Inference)\n",
    "\n",
    "### How Text Generation Works:\n",
    "\n",
    "```\n",
    "1. Start with seed: \"T\"\n",
    "2. Model predicts: \"o\" (90%), \"h\" (5%), \"a\" (5%)\n",
    "3. Sample from distribution (with temperature)\n",
    "4. Append to sequence: \"To\"\n",
    "5. Repeat → \"To be or not to be...\"\n",
    "```\n",
    "\n",
    "### Temperature Parameter:\n",
    "- **Low (0.3-0.7):** Conservative, repetitive\n",
    "- **Medium (0.8-1.0):** Balanced\n",
    "- **High (1.1-2.0):** Creative, random\n",
    "\n",
    "### Top-K Parameter:\n",
    "- Only sample from top K most likely tokens\n",
    "- Prevents choosing very unlikely characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "print(\"Testing your trained model...\\n\")\n",
    "!python inference.py --start \"T\" --steps 300 --temp 0.9 --top_k 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with Different Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare creativity levels\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONSERVATIVE (temp=0.5, top_k=10) - Focused, repetitive\")\n",
    "print(\"=\"*70)\n",
    "!python inference.py --start \"To\" --steps 200 --temp 0.5 --top_k 10\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BALANCED (temp=0.9, top_k=40) - Natural mix\")\n",
    "print(\"=\"*70)\n",
    "!python inference.py --start \"To\" --steps 200 --temp 0.9 --top_k 40\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CREATIVE (temp=1.3, top_k=60) - Diverse, surprising\")\n",
    "print(\"=\"*70)\n",
    "!python inference.py --start \"To\" --steps 200 --temp 1.3 --top_k 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Interactive Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive generation\n",
    "start_text = input(\"Enter starting text [default: 'W']: \") or \"W\"\n",
    "num_tokens = input(\"Tokens to generate [default: 250]: \") or \"250\"\n",
    "temperature = input(\"Temperature 0.1-2.0 [default: 0.9]: \") or \"0.9\"\n",
    "\n",
    "print(f\"\\nGenerating from '{start_text}'...\\n\")\n",
    "!python inference.py --start \"{start_text}\" --steps {num_tokens} --temp {temperature}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Analyze the Model's Behavior\n",
    "\n",
    "Let's peek inside the trained model to understand what it learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "ckpt = torch.load('checkpoints/out.pt', map_location='cpu')\n",
    "vocab = list(ckpt['meta']['vocab'])\n",
    "\n",
    "print(\"Model learned vocabulary:\")\n",
    "print(f\"  Total characters: {len(vocab)}\")\n",
    "print(f\"  Characters: {''.join(vocab)}\")\n",
    "\n",
    "# Visualize token embeddings\n",
    "trained_model = GPT(\n",
    "    vocab_size=len(vocab),\n",
    "    block_size=ckpt['meta']['block_size'],\n",
    "    n_layer=ckpt['meta']['n_layer'],\n",
    "    n_head=ckpt['meta']['n_head'],\n",
    "    n_embd=ckpt['meta']['n_embd']\n",
    ").eval()\n",
    "trained_model.load_state_dict(ckpt['model'])\n",
    "\n",
    "print(f\"\\nModel configuration:\")\n",
    "print(f\"  Block size (context length): {ckpt['meta']['block_size']}\")\n",
    "print(f\"  Number of layers: {ckpt['meta']['n_layer']}\")\n",
    "print(f\"  Number of heads: {ckpt['meta']['n_head']}\")\n",
    "print(f\"  Embedding dimension: {ckpt['meta']['n_embd']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What You Learned\n",
    "\n",
    "### Architecture Components:\n",
    "1. ✅ **Token Embedding** - Characters → vectors\n",
    "2. ✅ **Positional Embedding** - Add position info\n",
    "3. ✅ **Multi-Head Attention** - QKV mechanism\n",
    "4. ✅ **Layer Normalization** - Stabilize training\n",
    "5. ✅ **Residual Connections** - Gradient flow\n",
    "6. ✅ **Feed-Forward Network** - Non-linear processing\n",
    "7. ✅ **Dropout** - Regularization\n",
    "\n",
    "### Training:\n",
    "- ✅ Cross-entropy loss\n",
    "- ✅ AdamW optimizer\n",
    "- ✅ Gradient descent\n",
    "\n",
    "### Generation:\n",
    "- ✅ Autoregressive sampling\n",
    "- ✅ Temperature scaling\n",
    "- ✅ Top-K filtering\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Want to go deeper?\n",
    "\n",
    "1. **Modify architecture** - Change `config.py` parameters\n",
    "2. **Visualize attention** - See what the model focuses on\n",
    "3. **Try different datasets** - Train on other text\n",
    "4. **Scale up** - Larger models, more data\n",
    "\n",
    "---\n",
    "\n",
    "**Resources:**\n",
    "- [GitHub Repo](https://github.com/muhdaldiansyah/strivelm)\n",
    "- [Attention Paper](https://arxiv.org/abs/1706.03762) - \"Attention is All You Need\"\n",
    "- [GPT-3 Paper](https://arxiv.org/abs/2005.14165) - Language Models are Few-Shot Learners"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
